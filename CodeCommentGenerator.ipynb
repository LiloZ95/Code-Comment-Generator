{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "21d6a1c9542d4a04a682aca495ae73c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a408837b8b774acf94798d174e46d5ba",
              "IPY_MODEL_44bda440a6264a318e24b262dc885705",
              "IPY_MODEL_3a31d74f5e974382916c2fdefa35c233"
            ],
            "layout": "IPY_MODEL_360c0b447e364e779c0b14c61abd042a"
          }
        },
        "a408837b8b774acf94798d174e46d5ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e246b3da9eaf42a6936ff56086abb90e",
            "placeholder": "​",
            "style": "IPY_MODEL_60118e9225324e518a8c04c82918e813",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "44bda440a6264a318e24b262dc885705": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e80664eed6e4e2294a5bde42f95b7fe",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a120d5a6dfa14e74af46a5d8ba42681a",
            "value": 2
          }
        },
        "3a31d74f5e974382916c2fdefa35c233": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0f8118f4cff44806854c05278d1be0e3",
            "placeholder": "​",
            "style": "IPY_MODEL_592c886e03254557beb36703d04adacb",
            "value": " 2/2 [01:22&lt;00:00, 37.50s/it]"
          }
        },
        "360c0b447e364e779c0b14c61abd042a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e246b3da9eaf42a6936ff56086abb90e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60118e9225324e518a8c04c82918e813": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1e80664eed6e4e2294a5bde42f95b7fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a120d5a6dfa14e74af46a5d8ba42681a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0f8118f4cff44806854c05278d1be0e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "592c886e03254557beb36703d04adacb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 💬 Code Comment Generator  \n",
        "**LLM-Powered Code Comment Generator**  \n",
        "**Author:** Khalil Kurdi  \n",
        "\n",
        "---\n",
        "\n",
        "## 🧠 Overview\n",
        "\n",
        "This project explores automatic generation of concise, meaningful code comments using **pre-trained language models** (LLMs) such as **Phi-1** and **CodeLlama**.\n",
        "\n",
        "Rather than relying on fine-tuning — which proved ineffective and logically inconsistent in early experiments — we leverage **instruction-tuned LLMs** for zero-shot comment generation. The final output includes:\n",
        "\n",
        "- A simple **Gradio interface** for interactive use  \n",
        "- Support for **Phi-1** and **CodeLlama-7B-Instruct**  \n",
        "- Evaluation using **BLEU Score** to benchmark output quality  \n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "L0_mmEIoT0pU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Loading"
      ],
      "metadata": {
        "id": "waM06uaATNbg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers accelerate bitsandbytes\n",
        "!pip install gradio\n",
        "import gradio as gr\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtflVkXkU84_",
        "outputId": "1d4a7686-7106-4d11-c4ad-0f3604ad81ea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.38.1)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: brotli>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.1.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.116.1)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.1)\n",
            "Requirement already satisfied: gradio-client==1.11.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.11.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx<1.0,>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.4)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.11.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (25.0)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.3.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.4)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.47.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.35.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.11.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (2025.7.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.5.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CodeLlama\n",
        "llama_model_id = \"codellama/CodeLlama-7b-Instruct-hf\"\n",
        "llama_tokenizer = AutoTokenizer.from_pretrained(llama_model_id)\n",
        "llama_model = AutoModelForCausalLM.from_pretrained(\n",
        "    llama_model_id,\n",
        "    device_map=\"auto\",\n",
        "    load_in_4bit=True\n",
        ")\n",
        "llama_generator = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=llama_model,\n",
        "    tokenizer=llama_tokenizer,\n",
        "    max_new_tokens=128,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    repetition_penalty=1.1\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195,
          "referenced_widgets": [
            "21d6a1c9542d4a04a682aca495ae73c0",
            "a408837b8b774acf94798d174e46d5ba",
            "44bda440a6264a318e24b262dc885705",
            "3a31d74f5e974382916c2fdefa35c233",
            "360c0b447e364e779c0b14c61abd042a",
            "e246b3da9eaf42a6936ff56086abb90e",
            "60118e9225324e518a8c04c82918e813",
            "1e80664eed6e4e2294a5bde42f95b7fe",
            "a120d5a6dfa14e74af46a5d8ba42681a",
            "0f8118f4cff44806854c05278d1be0e3",
            "592c886e03254557beb36703d04adacb"
          ]
        },
        "id": "KnLssrKBHCXZ",
        "outputId": "65d10a6f-fba5-49be-fb60-a0253ee0195e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21d6a1c9542d4a04a682aca495ae73c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
        "\n",
        "model_id = \"microsoft/phi-1\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
        "phi_generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gis4drUoHCSB",
        "outputId": "56c086e7-11f3-42ab-ab79-eb6ff639efbb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Functions to Use"
      ],
      "metadata": {
        "id": "1V4g6l4rTUcP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate comment functions\n",
        "def generate_comment_codellama(code_snippet: str) -> str:\n",
        "    prompt = f\"[INST]generate a summarized comment for the following code:\\n\\n{code_snippet}\\n[/INST]\"\n",
        "    result = llama_generator(prompt)[0]['generated_text']\n",
        "    return result.split(\"[/INST]\")[-1].strip() if \"[/INST]\" in result else result.strip()\n",
        "\n",
        "def generate_comment_phi(code_snippet):\n",
        "    prompt = f\"# Python function:\\n{code_snippet}\\n# This function\"\n",
        "    output = phi_generator(prompt, max_new_tokens=60, do_sample=True, temperature=0.7)[0][\"generated_text\"]\n",
        "    return output.split(\" This function\")[-1].strip().split(\"\\n\")[0]\n",
        "\n",
        "# Unified handler\n",
        "def generate_comment(code, model_choice):\n",
        "    if model_choice == \"CodeLlama\":\n",
        "        comment = generate_comment_codellama(code)\n",
        "    else:\n",
        "        comment = generate_comment_phi(code)\n",
        "    return f\"# {comment.strip()}\"\n",
        "\n"
      ],
      "metadata": {
        "id": "7DNRk_dmHCI-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradio UI to try both Models"
      ],
      "metadata": {
        "id": "ZjtAh-FCTIdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradio UI\n",
        "gr.Interface(\n",
        "    fn=generate_comment,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=10, label=\"Paste your Python function here\", placeholder=\"def is_even(n):\\n    return n % 2 == 0\"),\n",
        "        gr.Radio([\"CodeLlama\", \"Phi-1\"], label=\"Choose model\", value=\"CodeLlama\")\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Generated Comment\"),\n",
        "    title=\"🔍 Code Comment Generator\",\n",
        "    theme=\"default\"\n",
        ").launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "BS6wTwLqHMGX",
        "outputId": "ac247541-869a-4d01-bbe0-7cc9c24e264f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://fdca278d358f2ce529.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://fdca278d358f2ce529.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 15 Samples testing"
      ],
      "metadata": {
        "id": "FBjWC_x3TDiI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "code_snippets = [\n",
        "    \"\"\"def factorial(n):\n",
        "    return 1 if n == 0 else n * factorial(n - 1)\"\"\",\n",
        "\n",
        "    \"\"\"def is_palindrome(s):\n",
        "    s = s.lower().replace(\" \", \"\")\n",
        "    return s == s[::-1]\"\"\",\n",
        "\n",
        "    \"\"\"def fibonacci(n):\n",
        "    a, b = 0, 1\n",
        "    for _ in range(n):\n",
        "        a, b = b, a + b\n",
        "    return a\"\"\",\n",
        "\n",
        "    \"\"\"def find_max(lst):\n",
        "    max_val = lst[0]\n",
        "    for val in lst:\n",
        "        if val > max_val:\n",
        "            max_val = val\n",
        "    return max_val\"\"\",\n",
        "\n",
        "    \"\"\"def reverse_string(s):\n",
        "    return s[::-1]\"\"\",\n",
        "\n",
        "    \"\"\"def count_vowels(s):\n",
        "    return sum(1 for c in s.lower() if c in 'aeiou')\"\"\",\n",
        "\n",
        "    \"\"\"def flatten(lst):\n",
        "    return [item for sublist in lst for item in sublist]\"\"\",\n",
        "\n",
        "    \"\"\"def read_lines(path):\n",
        "    with open(path, 'r') as f:\n",
        "        return f.readlines()\"\"\",\n",
        "\n",
        "    \"\"\"def is_anagram(s1, s2):\n",
        "    return sorted(s1.lower()) == sorted(s2.lower())\"\"\",\n",
        "\n",
        "    \"\"\"def get_unique(lst):\n",
        "    return list(set(lst))\"\"\",\n",
        "\n",
        "    \"\"\"def binary_search(arr, target):\n",
        "    left, right = 0, len(arr) - 1\n",
        "    while left <= right:\n",
        "        mid = (left + right) // 2\n",
        "        if arr[mid] == target:\n",
        "            return mid\n",
        "        elif arr[mid] < target:\n",
        "            left = mid + 1\n",
        "        else:\n",
        "            right = mid - 1\n",
        "    return -1\"\"\",\n",
        "\n",
        "    \"\"\"def merge_dicts(d1, d2):\n",
        "    return {**d1, **d2}\"\"\",\n",
        "\n",
        "    \"\"\"def is_prime(n):\n",
        "    if n <= 1:\n",
        "        return False\n",
        "    for i in range(2, int(n**0.5)+1):\n",
        "        if n % i == 0:\n",
        "            return False\n",
        "    return True\"\"\",\n",
        "\n",
        "    \"\"\"def dedup(seq):\n",
        "    seen = set()\n",
        "    return [x for x in seq if not (x in seen or seen.add(x))]\"\"\",\n",
        "\n",
        "    \"\"\"def average(numbers):\n",
        "    return sum(numbers) / len(numbers) if numbers else 0\"\"\"\n",
        "]\n",
        "\n",
        "# Run and print results\n",
        "for i, code in enumerate(code_snippets, 1):\n",
        "    print(f\"\\n🔢 Function {i}\")\n",
        "    print(\"📦 Code:\")\n",
        "    print(code)\n",
        "    print(\"\\n🧠 Generated Comment:\")\n",
        "    print(generate_comment_phi(code))\n"
      ],
      "metadata": {
        "id": "0SWnCjGJQrYf",
        "outputId": "311bf02a-1b32-4838-ce2b-c42ea240f548",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🔢 Function 1\n",
            "📦 Code:\n",
            "def factorial(n):\n",
            "    return 1 if n == 0 else n * factorial(n - 1)\n",
            "\n",
            "🧠 Generated Comment:\n",
            "calculates the factorial of a given number n recursively.\n",
            "\n",
            "🔢 Function 2\n",
            "📦 Code:\n",
            "def is_palindrome(s):\n",
            "    s = s.lower().replace(\" \", \"\")\n",
            "    return s == s[::-1]\n",
            "\n",
            "🧠 Generated Comment:\n",
            "takes a string as input and returns a boolean value indicating whether the string is a palindrome or not. The function removes all non-alphabetic characters and converts the string to lowercase before checking if it is equal to its reverse. If the string is a palindrome, the function returns\n",
            "\n",
            "🔢 Function 3\n",
            "📦 Code:\n",
            "def fibonacci(n):\n",
            "    a, b = 0, 1\n",
            "    for _ in range(n):\n",
            "        a, b = b, a + b\n",
            "    return a\n",
            "\n",
            "🧠 Generated Comment:\n",
            "generates the first n numbers of the Fibonacci sequence and checks if any of them are prime.\n",
            "\n",
            "🔢 Function 4\n",
            "📦 Code:\n",
            "def find_max(lst):\n",
            "    max_val = lst[0]\n",
            "    for val in lst:\n",
            "        if val > max_val:\n",
            "            max_val = val\n",
            "    return max_val\n",
            "\n",
            "🧠 Generated Comment:\n",
            "takes a list of integers as input and returns the maximum value in the list.\n",
            "\n",
            "🔢 Function 5\n",
            "📦 Code:\n",
            "def reverse_string(s):\n",
            "    return s[::-1]\n",
            "\n",
            "🧠 Generated Comment:\n",
            "takes a string as input and returns the reverse of that string.\n",
            "\n",
            "🔢 Function 6\n",
            "📦 Code:\n",
            "def count_vowels(s):\n",
            "    return sum(1 for c in s.lower() if c in 'aeiou')\n",
            "\n",
            "🧠 Generated Comment:\n",
            "takes a string as input and returns the count of vowels in it.\n",
            "\n",
            "🔢 Function 7\n",
            "📦 Code:\n",
            "def flatten(lst):\n",
            "    return [item for sublist in lst for item in sublist]\n",
            "\n",
            "🧠 Generated Comment:\n",
            "takes a list of lists as input and returns a flattened list.\n",
            "\n",
            "🔢 Function 8\n",
            "📦 Code:\n",
            "def read_lines(path):\n",
            "    with open(path, 'r') as f:\n",
            "        return f.readlines()\n",
            "\n",
            "🧠 Generated Comment:\n",
            "takes a path to a file as input and returns a list of strings, where each string represents a line in the file.\n",
            "\n",
            "🔢 Function 9\n",
            "📦 Code:\n",
            "def is_anagram(s1, s2):\n",
            "    return sorted(s1.lower()) == sorted(s2.lower())\n",
            "\n",
            "🧠 Generated Comment:\n",
            "takes two strings as input and returns True if they are anagrams of each other, and False otherwise.\n",
            "\n",
            "🔢 Function 10\n",
            "📦 Code:\n",
            "def get_unique(lst):\n",
            "    return list(set(lst))\n",
            "\n",
            "🧠 Generated Comment:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "takes a list as input and returns a new list with only the unique elements of the input list.\n",
            "\n",
            "🔢 Function 11\n",
            "📦 Code:\n",
            "def binary_search(arr, target):\n",
            "    left, right = 0, len(arr) - 1\n",
            "    while left <= right:\n",
            "        mid = (left + right) // 2\n",
            "        if arr[mid] == target:\n",
            "            return mid\n",
            "        elif arr[mid] < target:\n",
            "            left = mid + 1\n",
            "        else:\n",
            "            right = mid - 1\n",
            "    return -1\n",
            "\n",
            "🧠 Generated Comment:\n",
            "takes in a sorted list of integers and a target integer, and returns the index of the target integer in the list if it exists, otherwise returns -1.\n",
            "\n",
            "🔢 Function 12\n",
            "📦 Code:\n",
            "def merge_dicts(d1, d2):\n",
            "    return {**d1, **d2}\n",
            "\n",
            "🧠 Generated Comment:\n",
            "takes two dictionaries as input and returns a new dictionary that contains all the key-value pairs from both input dictionaries. If a key exists in both input dictionaries, the value from d2 will overwrite the value from d1.\n",
            "\n",
            "🔢 Function 13\n",
            "📦 Code:\n",
            "def is_prime(n):\n",
            "    if n <= 1:\n",
            "        return False\n",
            "    for i in range(2, int(n**0.5)+1):\n",
            "        if n % i == 0:\n",
            "            return False\n",
            "    return True\n",
            "\n",
            "🧠 Generated Comment:\n",
            "takes an integer n as input and returns True if n is a prime number, and False otherwise.\n",
            "\n",
            "🔢 Function 14\n",
            "📦 Code:\n",
            "def dedup(seq):\n",
            "    seen = set()\n",
            "    return [x for x in seq if not (x in seen or seen.add(x))]\n",
            "\n",
            "🧠 Generated Comment:\n",
            "takes a sequence as input and returns a new sequence with duplicates removed.\n",
            "\n",
            "🔢 Function 15\n",
            "📦 Code:\n",
            "def average(numbers):\n",
            "    return sum(numbers) / len(numbers) if numbers else 0\n",
            "\n",
            "🧠 Generated Comment:\n",
            "takes in a list of numbers and returns the average of those numbers. If the list is empty, it returns 0.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BLEU Evaluation"
      ],
      "metadata": {
        "id": "9dtAMRhKS_hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n"
      ],
      "metadata": {
        "id": "_wsprPTRRLmE",
        "outputId": "e15ca5ea-7c20-4814-b6b4-61f2aedfae7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "smoothie = SmoothingFunction().method4\n"
      ],
      "metadata": {
        "id": "zkhP5CCGRLjr"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_bleu(reference_comments, generated_comments):\n",
        "    scores = []\n",
        "    for ref, gen in zip(reference_comments, generated_comments):\n",
        "        ref_tokens = [ref.strip().split()]\n",
        "        gen_tokens = gen.strip().split()\n",
        "        score = sentence_bleu(ref_tokens, gen_tokens, smoothing_function=smoothie)\n",
        "        scores.append(score)\n",
        "    avg_score = sum(scores) / len(scores)\n",
        "    return avg_score\n"
      ],
      "metadata": {
        "id": "u0hbLbSORLhZ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reference_comments = [\n",
        "    \"Calculates the factorial of a number recursively.\",\n",
        "    \"Sorts a list using bubble sort algorithm.\",\n",
        "    \"Finds the maximum element in an array.\",\n",
        "    \"TODO\"\n",
        "]\n",
        "\n",
        "generated_comments = [\n",
        "    \"Returns the factorial of a number using recursion.\",\n",
        "    \"Implements bubble sort to arrange elements.\",\n",
        "    \"Searches for the maximum value in a list.\",\n",
        "    \"TODO\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "OPR3I_vbRPRf"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bleu_score = evaluate_bleu(reference_comments, generated_comments)\n",
        "print(f\"Average BLEU score: {bleu_score:.4f}\")\n"
      ],
      "metadata": {
        "id": "5ZXJEX70RPPj",
        "outputId": "e10b53ae-e355-420e-ce73-b92a50becc03",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average BLEU score: 0.4195\n"
          ]
        }
      ]
    }
  ]
}